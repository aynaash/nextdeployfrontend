import * as pulumi from "@pulumi/pulumi";
import * as inputs from "./types/input";
import * as outputs from "./types/output";
/**
 * The Flink Table resource allows the creation and management of Aiven Tables.
 *
 * ## Example Usage
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as aiven from "@pulumi/aiven";
 *
 * const table = new aiven.FlinkJobTable("table", {
 *     project: data.aiven_project.pr1.project,
 *     serviceName: aiven_flink.flink.service_name,
 *     tableName: "<TABLE_NAME>",
 *     integrationId: aiven_service_integration.flink_kafka.service_id,
 *     jdbcTable: "<JDBC_TABLE_NAME>",
 *     kafkaTopic: aiven_kafka_topic.table_topic.topic_name,
 *     schemaSql: `      \`+"\`cpu\`"+\` INT,
 *       \`+"\`node\`"+\` INT,
 *       \`+"\`occurred_at\`"+\` TIMESTAMP(3) METADATA FROM 'timestamp',
 *       WATERMARK FOR \`+"\`occurred_at\`"+\` AS \`+"\`occurred_at\`"+\` - INTERVAL '5' SECOND
 * `,
 * });
 * ```
 *
 * ## Import
 *
 * ```sh
 *  $ pulumi import aiven:index/flinkJobTable:FlinkJobTable table project/service_name/table_id
 * ```
 */
export declare class FlinkJobTable extends pulumi.CustomResource {
    /**
     * Get an existing FlinkJobTable resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    static get(name: string, id: pulumi.Input<pulumi.ID>, state?: FlinkJobTableState, opts?: pulumi.CustomResourceOptions): FlinkJobTable;
    /**
     * Returns true if the given object is an instance of FlinkJobTable.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    static isInstance(obj: any): obj is FlinkJobTable;
    /**
     * The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly integrationId: pulumi.Output<string>;
    /**
     * Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly jdbcTable: pulumi.Output<string | undefined>;
    /**
     * When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly kafkaConnectorType: pulumi.Output<string | undefined>;
    /**
     * Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly kafkaKeyFields: pulumi.Output<string[] | undefined>;
    /**
     * Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly kafkaKeyFormat: pulumi.Output<string | undefined>;
    /**
     * Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly kafkaStartupMode: pulumi.Output<string | undefined>;
    /**
     * Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly kafkaTopic: pulumi.Output<string | undefined>;
    /**
     * Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly kafkaValueFieldsInclude: pulumi.Output<string | undefined>;
    /**
     * Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly kafkaValueFormat: pulumi.Output<string | undefined>;
    /**
     * [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly likeOptions: pulumi.Output<string | undefined>;
    /**
     * For an OpenSearch table, the OpenSearch index the table outputs to. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly opensearchIndex: pulumi.Output<string | undefined>;
    /**
     * Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly project: pulumi.Output<string>;
    /**
     * The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly schemaSql: pulumi.Output<string>;
    /**
     * Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly serviceName: pulumi.Output<string>;
    /**
     * The Table ID of the flink table in the flink service.
     */
    readonly tableId: pulumi.Output<string>;
    /**
     * Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
     */
    readonly tableName: pulumi.Output<string>;
    /**
     * Kafka upsert connector configuration.
     */
    readonly upsertKafka: pulumi.Output<outputs.FlinkJobTableUpsertKafka | undefined>;
    /**
     * Create a FlinkJobTable resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: FlinkJobTableArgs, opts?: pulumi.CustomResourceOptions);
}
/**
 * Input properties used for looking up and filtering FlinkJobTable resources.
 */
export interface FlinkJobTableState {
    /**
     * The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    integrationId?: pulumi.Input<string>;
    /**
     * Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
     */
    jdbcTable?: pulumi.Input<string>;
    /**
     * When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaConnectorType?: pulumi.Input<string>;
    /**
     * Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaKeyFields?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaKeyFormat?: pulumi.Input<string>;
    /**
     * Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaStartupMode?: pulumi.Input<string>;
    /**
     * Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaTopic?: pulumi.Input<string>;
    /**
     * Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaValueFieldsInclude?: pulumi.Input<string>;
    /**
     * Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaValueFormat?: pulumi.Input<string>;
    /**
     * [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
     */
    likeOptions?: pulumi.Input<string>;
    /**
     * For an OpenSearch table, the OpenSearch index the table outputs to. This property cannot be changed, doing so forces recreation of the resource.
     */
    opensearchIndex?: pulumi.Input<string>;
    /**
     * Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    project?: pulumi.Input<string>;
    /**
     * The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
     */
    schemaSql?: pulumi.Input<string>;
    /**
     * Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    serviceName?: pulumi.Input<string>;
    /**
     * The Table ID of the flink table in the flink service.
     */
    tableId?: pulumi.Input<string>;
    /**
     * Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
     */
    tableName?: pulumi.Input<string>;
    /**
     * Kafka upsert connector configuration.
     */
    upsertKafka?: pulumi.Input<inputs.FlinkJobTableUpsertKafka>;
}
/**
 * The set of arguments for constructing a FlinkJobTable resource.
 */
export interface FlinkJobTableArgs {
    /**
     * The id of the service integration that is used with this table. It must have the service integration type `flink`. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    integrationId: pulumi.Input<string>;
    /**
     * Name of the jdbc table that is to be connected to this table. Valid if the service integration id refers to a mysql or postgres service. This property cannot be changed, doing so forces recreation of the resource.
     */
    jdbcTable?: pulumi.Input<string>;
    /**
     * When used as a source, upsert Kafka connectors update values that use an existing key and delete values that are null. For sinks, the connector correspondingly writes update or delete messages in a compacted topic. If no matching key is found, the values are added as new entries. For more information, see the Apache Flink documentation The possible values are `kafka` and `upsert-kafka`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaConnectorType?: pulumi.Input<string>;
    /**
     * Defines an explicit list of physical columns from the table schema that configure the data type for the key format. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaKeyFields?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Kafka Key Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaKeyFormat?: pulumi.Input<string>;
    /**
     * Startup mode The possible values are `earliest-offset`, `latest-offset`, `group-offsets` and `timestamp`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaStartupMode?: pulumi.Input<string>;
    /**
     * Name of the kafka topic that is to be connected to this table. Valid if the service integration id refers to a kafka service. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaTopic?: pulumi.Input<string>;
    /**
     * Controls how key columns are handled in the message value. Select ALL to include the physical columns of the table schema in the message value. Select EXCEPT_KEY to exclude the physical columns of the table schema from the message value. This is the default for upsert Kafka connectors. The possible values are `[ALL EXCEPT_KEY]`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaValueFieldsInclude?: pulumi.Input<string>;
    /**
     * Kafka Value Format The possible values are `avro`, `avro-confluent`, `debezium-avro-confluent`, `debezium-json` and `json`. This property cannot be changed, doing so forces recreation of the resource.
     */
    kafkaValueFormat?: pulumi.Input<string>;
    /**
     * [LIKE](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/create/#like) statement for table creation. This property cannot be changed, doing so forces recreation of the resource.
     */
    likeOptions?: pulumi.Input<string>;
    /**
     * For an OpenSearch table, the OpenSearch index the table outputs to. This property cannot be changed, doing so forces recreation of the resource.
     */
    opensearchIndex?: pulumi.Input<string>;
    /**
     * Identifies the project this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    project: pulumi.Input<string>;
    /**
     * The SQL statement to create the table. This property cannot be changed, doing so forces recreation of the resource.
     */
    schemaSql: pulumi.Input<string>;
    /**
     * Specifies the name of the service that this resource belongs to. To set up proper dependencies please refer to this variable as a reference. This property cannot be changed, doing so forces recreation of the resource.
     */
    serviceName: pulumi.Input<string>;
    /**
     * Specifies the name of the table. This property cannot be changed, doing so forces recreation of the resource.
     */
    tableName: pulumi.Input<string>;
    /**
     * Kafka upsert connector configuration.
     */
    upsertKafka?: pulumi.Input<inputs.FlinkJobTableUpsertKafka>;
}
